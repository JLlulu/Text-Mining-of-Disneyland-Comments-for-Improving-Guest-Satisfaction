# -*- coding: utf-8 -*-
"""Text Mining of Disneyland Comments for Improving Guest Satisfaction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10C8j0yXq_ObaOnPfKXEsVFngE9o3Mbig

# **Data cleaning and Preperation**

**Install the modules we need and define the text normalization function**
"""

# Commented out IPython magic to ensure Python compatibility.
#the module 'sys' allows istalling module from inside Jupyter
import sys

!{sys.executable} -m pip install numpy
import numpy as np

!{sys.executable} -m pip install pandas
import pandas as pd

#Natrual Language ToolKit (NLTK)
!{sys.executable} -m pip install nltk
import nltk

!{sys.executable} -m pip install sklearn
from sklearn import metrics
#from sklearn.model_selection import GridSearchCV
from sklearn.feature_extraction.text import  CountVectorizer #bag-of-words vectorizer
from sklearn.decomposition import LatentDirichletAllocation #package for LDA

# Plotting tools

from pprint import pprint
#!{sys.executable} -m pip install pyLDAvis #visualizing LDA
#import pyLDAvis
#import pyLDAvis.lda_model

import matplotlib.pyplot as plt
# %matplotlib inline

#define text normalization function
# %run ./Text_Normalization_Function.ipynb #defining text normalization function

#ignore warnings about future changes in functions as they take too much space
import warnings
warnings.simplefilter(action='ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

"""**Load data.**"""

df = pd.read_csv("DisneylandReviews-modified.csv", sep=',', header=0, encoding='latin-1')
df.head()

"""**Normalize the review text**"""

normalized_reviews = normalize_corpus(df["Review_Text"])
df_normalized = df
df_normalized["Review_Text"] = normalized_reviews
df_normalized.head()

"""**Create polarity column for the dataset. (>=3 positive, <=2 negative, null 0)**"""

df_normalized["Polarity"] = 0
df_normalized["Polarity"].loc[df_normalized["Rating"] >= 3] = "positive"
df_normalized["Polarity"].loc[(df_normalized["Rating"] >= 1) & (df_normalized["Rating"] <=2)] = "negative"
df_normalized.head()

"""# **Sentiment Analysis**

**Senetiment analysis training and testing data separation.**
"""

nltk.download('vader_lexicon')
from nltk.sentiment.vader import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

from pandas.core.common import random_state
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer(ngram_range = (1,2))

train_data = df_normalized.loc[df_normalized["Polarity"] != 0].sample(frac = 0.8, random_state = 1)
test_data = df_normalized.loc[df_normalized["Polarity"] != 0].drop(train_data.index)
unprocessed_data = df_normalized.loc[df_normalized["Polarity"] == 0]

normalized_train_reviews = train_data["Review_Text"]
normalized_test_reviews = test_data["Review_Text"]

train_polarity = train_data["Polarity"]
test_polarity = test_data["Polarity"]

feature_matrix_TRAIN = vectorizer.fit_transform(normalized_train_reviews).astype(float)
feature_matrix_TEST = vectorizer.transform(normalized_test_reviews)

unprocessed_data.shape[0]

"""**Define sentiment analysis function.**"""

def analyze_sentiment_vader_lexicon(review, threshold = 0.1, verbose = False):
    scores = analyzer.polarity_scores(review)
    binary_sentiment = 'positive' if scores['compound'] >= threshold else 'negative'
    if verbose:
        print('VADER Polarity (Binary):', binary_sentiment)
        print('VADER Score:', round(scores['compound'], 2))
    return binary_sentiment,scores['compound']

"""**Apply the function.**"""

VADER_polarity_test = [analyze_sentiment_vader_lexicon(review, threshold=0.1) for review in normalized_test_reviews]
VADER_polarity_test_df = pd.DataFrame(VADER_polarity_test, columns = ['VADER Polarity','VADER Score'])
VADER_polarity_test_df.head()

"""**Compute confusion matrix**"""

from sklearn import metrics

pd.crosstab(pd.Series(test_polarity),
            pd.Series(VADER_polarity_test_df['VADER Polarity']),
            rownames = ['True:'],
            colnames = ['Predicted:'],
            margins = True)

"""**Adjust threshould parameter.**"""

def try_threshold_for_accuracy(sentiment_scores, threshold_for_pos):
    VADER_binary_polarity = ['positive' if s >= threshold_for_pos else 'negative' for s in list(sentiment_scores)]
    accuracy = metrics.accuracy_score(test_polarity, VADER_binary_polarity)
    return(accuracy)

thresholds = np.linspace(-1,1,1000)
acc_rates = [try_threshold_for_accuracy(VADER_polarity_test_df['VADER Score'],threshold) for threshold in thresholds]

best_threshold = thresholds[acc_rates.index(max(acc_rates))]
best_threshold

max(acc_rates)

"""**Assign polarity to reviews without ratings.**"""

VADER_polarity_test_unprocessed = [analyze_sentiment_vader_lexicon(review, threshold=best_threshold) for review in unprocessed_data["Review_Text"]]
VADER_polarity_test_unprocessed_df = pd.DataFrame(VADER_polarity_test_unprocessed, columns = ['VADER Polarity','VADER Score'])
VADER_polarity_test_unprocessed_df

"""**Combine the predicted polarity to original dataset.**"""

unprocessed_data_1 = unprocessed_data.reset_index()
unprocessed_data_1["Polarity"] = VADER_polarity_test_unprocessed_df["VADER Polarity"]
df_complete = pd.concat([df_normalized.loc[df_normalized["Polarity"] != 0], unprocessed_data_1])
df_complete = df_complete.loc[:, df_complete.columns!="index"]
df_complete

"""# **Time Series Analysis**

**Separating dataset by branch.**
"""

df_complete_HK_polarity = df_complete.loc[df_complete["Branch"] == "Disneyland_HongKong"][["Year_Month","Polarity"]]
df_complete_HK_polarity["Polarity"] = df_complete_HK_polarity["Polarity"] == "positive"

df_complete_Paris_polarity = df_complete.loc[df_complete["Branch"] == "Disneyland_Paris"][["Year_Month","Polarity"]]
df_complete_Paris_polarity["Polarity"] = df_complete_Paris_polarity["Polarity"] == "positive"

df_complete_California_polarity = df_complete.loc[df_complete["Branch"] == "Disneyland_California"][["Year_Month","Polarity"]]
df_complete_California_polarity["Polarity"] = df_complete_California_polarity["Polarity"] == "positive"

df_complete_HK_polarity.head()

"""**Group data by time.**"""

from datetime import datetime

HK_Time_Rating = df_complete_HK_polarity.groupby("Year_Month").mean().reset_index()
HK_Time_Rating = HK_Time_Rating.loc[HK_Time_Rating["Year_Month"] != "missing"]
HK_Time_Rating["Year_Month"] = pd.to_datetime(HK_Time_Rating["Year_Month"])
HK_Time_Rating = HK_Time_Rating.sort_values("Year_Month")

Paris_Time_Rating = df_complete_Paris_polarity.groupby("Year_Month").mean().reset_index()
Paris_Time_Rating = Paris_Time_Rating.loc[Paris_Time_Rating["Year_Month"] != "missing"]
Paris_Time_Rating["Year_Month"] = pd.to_datetime(HK_Time_Rating["Year_Month"])
Paris_Time_Rating = Paris_Time_Rating.sort_values("Year_Month")

California_Time_Rating = df_complete_California_polarity.groupby("Year_Month").mean().reset_index()
California_Time_Rating = California_Time_Rating.loc[California_Time_Rating["Year_Month"] != "missing"]
California_Time_Rating["Year_Month"] = pd.to_datetime(HK_Time_Rating["Year_Month"])
California_Time_Rating = California_Time_Rating.sort_values("Year_Month")

HK_Time_Rating

"""**Create line graphs.**"""

plt.plot(HK_Time_Rating["Year_Month"], HK_Time_Rating["Polarity"], label = "HK")
plt.plot(Paris_Time_Rating["Year_Month"], Paris_Time_Rating["Polarity"], label = "Paris")
plt.plot(California_Time_Rating["Year_Month"], California_Time_Rating["Polarity"], label = "California")

plt.xlabel("Time")
plt.ylabel('Percentage of Positive Reviews')
plt.title('Percentage of Positive Reviews in Different Disneylands')
plt.legend()
plt.show()

"""It seems overall, all three Disneylands are improving their services on average.

# **Topic Modelling Analysis**
"""

def display_topics(model, feature_names, no_top_words):
    for topic_idx, topic in enumerate(model.components_):
        print("Topic %d:" % (topic_idx))
        print(" ".join([feature_names[i]
                        for i in topic.argsort()[:-no_top_words - 1:-1]]))

def get_topic_words(vectorizer, lda_model, n_words):
    keywords = np.array(vectorizer.get_feature_names_out())
    topic_words = []
    for topic_weights in lda_model.components_:
        top_word_locs = (-topic_weights).argsort()[:n_words]
        topic_words.append(keywords.take(top_word_locs).tolist())
    return topic_words

"""**Set up stop words**"""

from nltk.corpus import stopwords
nltk.download('stopwords')

stop_words=set(stopwords.words("english"))
print(stop_words)

add_stopwords ={'disneyland','disney','world','park','hk','hong','kong'}
stop_words_new = add_stopwords.union(stop_words)
print(stop_words_new)

"""**Filter out bad reviews.**"""

df_good = df_complete.loc[df_complete["Polarity"] =="positive"]
df_normalized_bad = df_complete.loc[df_complete["Polarity"] =="negative"]
df_normalized_bad

"""**Split review for each Disney park.**

**(1) Disneyland Hong Kong**
"""

df_bad_HK=df_normalized_bad[df_normalized_bad['Branch']=="Disneyland_HongKong"]
words_bad_HK=[]
for index,row in df_bad_HK.iterrows():
  review=row["Review_Text"]
  #print(review)
  review_words=nltk.word_tokenize(review)
  words_bad_HK.append(review_words)

"""Text Tokenization and Filtering
*   Tokenize by words
*   Filter stopwords
"""

words_bad_HK= [word for sublist in words_bad_HK for word in sublist]
print(words_bad_HK)

filtered_tokens_HK=[]

for w in words_bad_HK:
    if w not in stop_words_new:
        filtered_tokens_HK.append(w)

print("Filterd Sentence (without stopwords):",filtered_tokens_HK)

"""**Topic Modelling for Disney Hong Kong**"""

#define a Bag-of-Words vecgtorizer
bow_vectorizer_news_HK = CountVectorizer(max_features=1000)

#vectorize data
bow_news_corpus_HK = bow_vectorizer_news_HK.fit_transform(filtered_tokens_HK)

lda_news_HK = LatentDirichletAllocation(n_components=6, max_iter=100,
                                     doc_topic_prior = 0.25,
                                     topic_word_prior = 1).fit(bow_news_corpus_HK)

no_top_words_news = 20
display_topics(lda_news_HK, bow_vectorizer_news_HK.get_feature_names_out(), no_top_words_news)

word_weights_HK = lda_news_HK.components_ / lda_news_HK.components_.sum(axis=1)[:, np.newaxis]
word_weights_df_HK = pd.DataFrame(word_weights_HK.T,
                               index = bow_vectorizer_news_HK.get_feature_names_out(),
                               columns = ["Topic_" + str(i) for i in range(6)])
word_weights_df_HK.sort_values(by='Topic_0',ascending=False).head(20)

"""**Find Dominant Topic in bad reviews for Disney Hong Kong**

Let's firstly create a Wordcloud plot to gain a general insight into common complaints about Disney Hong Kong.
"""

from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator

from wordcloud.wordcloud import random_color_func
bad_review_hk="".join(i for i in df_bad_HK["Review_Text"])
wordcloud_hk=WordCloud(background_color="white",stopwords=stop_words_new,width=800,height=800,min_font_size=10).generate(bad_review_hk)
plt.figure( figsize=(15,10))
plt.imshow(wordcloud_hk, interpolation='bilinear')
plt.axis("off")
plt.show()

reviews_list_HK = []
for index, row in df_bad_HK.iterrows():
    review_text = row['Review_Text']
    reviews_list_HK.append(review_text)
#print(reviews_list_HK)
bow_news_corpus_HK1 = bow_vectorizer_news_HK.fit_transform(reviews_list_HK)

lda_news_topic_weights_HK = lda_news_HK.transform(bow_news_corpus_HK1)

#array of document "names" and topic "names" ("names" are just indecies)
doc_names = ["Doc_" + str(i) for i in range(len(reviews_list_HK))]
topic_names = ["Topic_" + str(i) for i in range(6)]

#convert to dataframe
df_document_topic_HK = pd.DataFrame(np.round(lda_news_topic_weights_HK, 4), columns=topic_names, index=doc_names)
df_document_topic_HK.head(5)

#vector of indecies for columns with the highest value by each row in df_document_topic
dominant_topic_HK = np.argmax(df_document_topic_HK.values, axis=1)

#add dominant_topic as a column to df_document_topic
df_document_topic_HK['dominant_topic'] = dominant_topic_HK
df_document_topic_HK.head(5)

dominant_topic_counts = df_document_topic_HK.groupby(['dominant_topic']).size().reset_index(name='counts')
print(dominant_topic_counts)

"""Topic 1 is the dominant topic in bad reviews for Disney Hong Kong. It is about "Ticket prices, buying tickets, and value for money".

**(2) Disney California**
"""

df_bad_CA=df_normalized_bad[df_normalized_bad['Branch']=="Disneyland_California"]
words_bad_CA=[]
for index,row in df_bad_CA.iterrows():
  review=row["Review_Text"]
  #print(review)
  review_words=nltk.word_tokenize(review)
  words_bad_CA.append(review_words)

words_bad_CA= [word for sublist in words_bad_CA for word in sublist]
print(words_bad_CA)

filtered_tokens_CA=[]

for w in words_bad_CA:
    if w not in stop_words_new:
        filtered_tokens_CA.append(w)

print("Filterd Sentence (without stopwords):",filtered_tokens_CA)

"""**Topic Modelling for Disney California**"""

#define a Bag-of-Words vecgtorizer
bow_vectorizer_news_CA = CountVectorizer(max_features=1000)

#vectorize data
bow_news_corpus_CA = bow_vectorizer_news_CA.fit_transform(filtered_tokens_CA)

lda_news_CA = LatentDirichletAllocation(n_components=6, max_iter=100,
                                     doc_topic_prior = 0.25,
                                     topic_word_prior = 1).fit(bow_news_corpus_CA)

no_top_words_news = 20
display_topics(lda_news_CA, bow_vectorizer_news_CA.get_feature_names_out(), no_top_words_news)

word_weights_CA = lda_news_CA.components_ / lda_news_CA.components_.sum(axis=1)[:, np.newaxis]
word_weights_df_CA = pd.DataFrame(word_weights_CA.T,
                               index = bow_vectorizer_news_CA.get_feature_names_out(),
                               columns = ["Topic_" + str(i) for i in range(6)])
word_weights_df_CA.sort_values(by='Topic_0',ascending=False).head(20)

"""**Find Dominant Topic in bad reviews for Disney California**"""

from wordcloud.wordcloud import random_color_func
bad_review_ca="".join(i for i in df_bad_CA["Review_Text"])
wordcloud_ca=WordCloud(background_color="white",stopwords=stop_words_new,width=800,height=800,min_font_size=10).generate(bad_review_ca)
plt.figure( figsize=(15,10))
plt.imshow(wordcloud_ca, interpolation='bilinear')
plt.axis("off")
plt.show()

reviews_list_CA = []
for index, row in df_bad_CA.iterrows():
    review_text = row['Review_Text']
    reviews_list_CA.append(review_text)
#print(reviews_list_CA)
bow_news_corpus_CA1 = bow_vectorizer_news_CA.fit_transform(reviews_list_CA)

lda_news_topic_weights_CA = lda_news_CA.transform(bow_news_corpus_CA1)

#array of document "names" and topic "names" ("names" are just indecies)
doc_names = ["Doc_" + str(i) for i in range(len(reviews_list_CA))]
topic_names = ["Topic_" + str(i) for i in range(6)]

#convert to dataframe
df_document_topic_CA = pd.DataFrame(np.round(lda_news_topic_weights_CA, 4), columns=topic_names, index=doc_names)
df_document_topic_CA.head(5)

#vector of indecies for columns with the highest value by each row in df_document_topic
dominant_topic_CA = np.argmax(df_document_topic_CA.values, axis=1)

#add dominant_topic as a column to df_document_topic
df_document_topic_CA['dominant_topic'] = dominant_topic_CA
df_document_topic_CA.head(5)

dominant_topic_counts = df_document_topic_CA.groupby(['dominant_topic']).size().reset_index(name='counts')
print(dominant_topic_counts)

"""Topic 1 is the dominant topic in bad reviews for Disney California. It is about " Money spending, character, attraction, and staff behavior".

**(3) Disney Paris**
"""

df_bad_PR=df_normalized_bad[df_normalized_bad['Branch']=="Disneyland_Paris"]
words_bad_PR=[]
for index,row in df_bad_PR.iterrows():
  review=row["Review_Text"]
  #print(review)
  review_words=nltk.word_tokenize(review)
  words_bad_PR.append(review_words)

words_bad_PR= [word for sublist in words_bad_PR for word in sublist]
print(words_bad_PR)

filtered_tokens_PR=[]

for w in words_bad_PR:
    if w not in stop_words_new:
        filtered_tokens_PR.append(w)

print("Filterd Sentence (without stopwords):",filtered_tokens_PR)

"""**Topic Modelling for Disney Paris**"""

#define a Bag-of-Words vecgtorizer
bow_vectorizer_news_PR = CountVectorizer(max_features=1000)

#vectorize data
bow_news_corpus_PR = bow_vectorizer_news_PR.fit_transform(filtered_tokens_PR)

lda_news_PR = LatentDirichletAllocation(n_components=6, max_iter=100,
                                     doc_topic_prior = 0.25,
                                     topic_word_prior = 1).fit(bow_news_corpus_PR)

no_top_words_news = 20
display_topics(lda_news_PR, bow_vectorizer_news_PR.get_feature_names_out(), no_top_words_news)

word_weights_PR = lda_news_PR.components_ / lda_news_PR.components_.sum(axis=1)[:, np.newaxis]
word_weights_df_PR = pd.DataFrame(word_weights_PR.T,
                               index = bow_vectorizer_news_PR.get_feature_names_out(),
                               columns = ["Topic_" + str(i) for i in range(6)])
word_weights_df_PR.sort_values(by='Topic_0',ascending=False).head(20)

"""**Find Dominant Topic in Bad Reviews for Disney Paris**"""

from wordcloud.wordcloud import random_color_func
bad_review_pr="".join(i for i in df_bad_PR["Review_Text"])
wordcloud_pr=WordCloud(background_color="white",stopwords=stop_words_new,width=800,height=800,min_font_size=10).generate(bad_review_pr)
plt.figure( figsize=(15,10))
plt.imshow(wordcloud_pr, interpolation='bilinear')
plt.axis("off")
plt.show()

reviews_list_PR = []
for index, row in df_bad_PR.iterrows():
    review_text = row['Review_Text']
    reviews_list_PR.append(review_text)
#print(reviews_list_CA)
bow_news_corpus_PR1 = bow_vectorizer_news_PR.fit_transform(reviews_list_PR)

lda_news_topic_weights_PR = lda_news_PR.transform(bow_news_corpus_PR1)

#array of document "names" and topic "names" ("names" are just indecies)
doc_names = ["Doc_" + str(i) for i in range(len(reviews_list_PR))]
topic_names = ["Topic_" + str(i) for i in range(6)]

#convert to dataframe
df_document_topic_PR = pd.DataFrame(np.round(lda_news_topic_weights_PR, 4), columns=topic_names, index=doc_names)
df_document_topic_PR.head(5)

#vector of indecies for columns with the highest value by each row in df_document_topic
dominant_topic_PR = np.argmax(df_document_topic_PR.values, axis=1)

#add dominant_topic as a column to df_document_topic
df_document_topic_PR['dominant_topic'] = dominant_topic_PR
df_document_topic_PR.head(5)

dominant_topic_counts = df_document_topic_PR.groupby(['dominant_topic']).size().reset_index(name='counts')
print(dominant_topic_counts)

"""Topic 5 is the dominant topic in bad reviews for Disney Paris. It is about "Accommodations, hotels, and transportation"."""